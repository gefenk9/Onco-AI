# Example environment configuration file for onco-ai project
# Copy this file to .env and fill in your actual values

# --- LLM Provider Configuration ---
# Specify the LLM provider: "azure_openai", "bedrock", or "anthropic"
# If not set, defaults to "azure_openai"
LLM_PROVIDER=azure_openai

# --- Azure OpenAI Configuration (if LLM_PROVIDER is "azure_openai") ---
# Azure OpenAI API key - get this from your Azure OpenAI resource
AZURE_OPENAI_API_KEY=your_azure_openai_api_key_here

# Azure OpenAI endpoint URL - get this from your Azure OpenAI resource
AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com/

# OpenAI model to use - available models depend on your Azure deployment
OPEN_AI_MODEL=o4-mini

# --- AWS Bedrock Configuration (if LLM_PROVIDER is "bedrock") ---
# AWS region for Bedrock service
AWS_REGION=eu-west-1

# Bedrock model ID to use
BEDROCK_MODEL=eu.anthropic.claude-3-7-sonnet-20250219-v1:0

# --- Anthropic API Configuration (if LLM_PROVIDER is "anthropic") ---
# Your Anthropic API key. Required if LLM_PROVIDER is "anthropic".
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Anthropic model to use - available models from Anthropic API
ANTHROPIC_MODEL=claude-3-5-sonnet-20240620